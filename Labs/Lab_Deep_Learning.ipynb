{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab - Deep Learning**\n",
    "\n",
    "<b>ITM-370: Data Analytics</b><br>\n",
    "**Lecturer: HAS Sothea, PhD**\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** The goal of this practical lab is to introduce you to building Multilayer Perceptron (MLP) models. After this, you will be familar with how to get start with model architecture, what are the influences of activation functions, learning rate, batchsize... We will try to understand all these concepts using the classic handwriten digit `Mnist` which can be imported from `keras` module.\n",
    "\n",
    "> **The `Jupyter Notebook` for this Lab can be downloaded here: [Lab_Deep_Learning.ipynb](https://hassothea.github.io/Data_Analytics_AUPP/Labs/Lab_Deep_Learning.ipynb)**.\n",
    "\n",
    "> **Or you can work with this notebook in `Google Colab` here: [Lab_Deep_Learning.ipynb](https://colab.research.google.com/drive/1WMzYuQWRq5xZ8phMkhuRmaaYvc7cXDcn?usp=sharing)**.\n",
    "\n",
    "-----------\n",
    "\n",
    "## 1. Importing `Mnist` dataset\n",
    "\n",
    "You can import `Mnist` dataset from `keras` as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJbElEQVR4nO3cX6jXdx3H8e85Hv9ks21mWxssZ+pSNpuVlDbRII7toosiTjJ2ZXTR1jZWBqsR9AeLFRHYsl0Mlhu0WmcU7aI/SIQMptZaLFY0YyqxaZYePCtn6X7n20286CLQ93eec34eH4/r34vP9+LA83xuPgNt27YNADRNMzjdHwBA/xAFAEIUAAhRACBEAYAQBQBCFAAIUQAghs71h8ODI5P5HQBMsl0To2f9jZsCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxNN0fAGczMFT/M531pkWT8CXnx/OfubbTrjd/orxZvPRv5c382wfKm79+c05588yax8qbpmmaY72T5c17RreWN8s+vbe8mQncFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDCg3gzzKyVy8ubdu7s8ubwxsvKm1Nr6w+ZNU3TLLy0vnvyxm6Prc00P3tlQXnztW/fXN7sW/VoeXPwzKnypmma5r6jw+XN1U+2nc66GLkpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMRA27bn9FLU8ODIZH8L/6P3vnd22m3fuaO8uW72nE5nMbXOtL3y5r1fv7u8GTo5NY/HLXjp1U67ucfqD+m1Tz/X6ayZZtfE6Fl/46YAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQAxN9wfw/819/nCn3W//dU15c93so53Ommm2Hllb3hz456LyZufSx8ubpmma8Yn666VXfuupTmf1s6l5w/Xi5aYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEANt257T+1LDgyOT/S2cB2Nb1pU3L998sryZ9ftLyptnb7+/vOlq27G3lze/2Vh/3K53Yry8adfdWN40TdMcuqu+WXLLs53OYmbaNTF61t+4KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEB/FoZi16Y3nTOz5W3hx8tP5IXdM0zR82PFTevPurd5Y3V+x4qryBC4kH8QAoEQUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAghqb7A5h+vWPHp+ScMy/PmZJzmqZprr/1j+XN3x+YVT9oolffQB9zUwAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgvJLKlFl5z/5Ouy2r3l/efHfxL8ubjSOfLG8WPLa3vIF+5qYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEB7EY8r0Tox32h2/bWV585cnTpU3n932SHnzuY9+uLxpf3dpedM0TXPNV/bUR23b6SwuXm4KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADHQtuf2Ytbw4MhkfwucN2MfW1fefO8L3yhvlgzNK2+6uv6RO8qb5Q8eKW9ePXCovOHCsGti9Ky/cVMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACA/iwX+1N60ub95w34vlzfff+ovypqsVv/p4efO2L42XN70/HyhvmHoexAOgRBQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA8CAevAazrryivDm8eVmns/bds728Gezwf9+tBzeVN+Prj5c3TD0P4gFQIgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4ZVUuED88MU95c38gTnlzSvt6fLmg3feXd7M//G+8obXxiupAJSIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABBD0/0B0C8m1q8ub14YmVfe3LD6UHnTNN0et+vi/rF3lDfzf/L0JHwJ08FNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACA8iEffG1hzQ3mz/67643EP3vRwebNh3unyZir9uz1T3uwdW1I/aOJIfUNfclMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACA/i0cnQksXlzQtbru501hc3/6C8+cglxzqd1c/uPbqmvNm9fW15c/nDe8obZg43BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDwIN4MM3TtW8qb8XddVd5s/vLPy5tPXPaj8qbfbT1Sf3Buz3fqD9s1TdMs3Pnr8ubyCY/bUeOmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB4JXUKDF315vJm7KHXdzrrtiW7y5tbFhztdFY/u+Ol9eXNMw+sLm8WPf5cebPwH14upX+5KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDERf0g3ukPrKlvPjVW3ty77KflzabXnSxv+t3R3qlOuw1PbC1vVnz+T+XNwhP1h+omygvob24KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAHFRP4h36EP1Ju5fNToJX3L+7DixtLzZvntTeTPQGyhvVmw7WN40TdMsP7qvvOl1OglwUwAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACIgbZt23P54fDgyGR/CwCTaNfE2R/0dFMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGKgbdt2uj8CgP7gpgBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAED8BwdNKpY4Umj7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, label_train), (X_test, label_test) = mnist.load_data()\n",
    "\n",
    "# Visualize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0,:,:])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.** What's the dimension of `X_train` and `X_test`? \n",
    "\n",
    "- Can you guess what are `label_train` and `label_test`?\n",
    "\n",
    "- How many distinct values of these labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Visualize the first 8 images of the training data in two rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** Reshape the training data and testing data from 3 dimensional into 2 dimensional using `x.reshape()`, i.e., $n\\times 28\\times 28\\to n\\times 784$ where each row is a $784$-dimensional vector of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D.** Scale the training and testing inputs pixels to take values between $0$ and $1$. This helps the training process converges quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model \n",
    "\n",
    "Here, we build MLP model with the following architecture and hyperparameters:\n",
    "\n",
    "**A.** Designing a network\n",
    "\n",
    "- Network layers: $[d, 32, 32, M]$ where is the input dimension. What are the values of $d$ and $M$?\n",
    "- Activation function: **ReLU** and **Softmax** on the output layer.\n",
    "- Optimization method: **Adam**\n",
    "- Learning rate: $0.001$\n",
    "- Loss: **'sparse_categorical_crossentropy'** to be minimized.\n",
    "- Metric: **['accuracy']** is the metric to keep track the performance of the model during training process.\n",
    "- Minibatch size: $b=32$ (subset used to approximate the full loss function when training)\n",
    "- Number of epochs: **epchs = 50**.\n",
    "- Validation size: **0.2** for keeping track the model state during training.\n",
    "\n",
    "Let me set up all these for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "d = 784 # Input dimensions\n",
    "M = 10 # Number of classes\n",
    "\n",
    "model = Sequential()\n",
    "# input layer\n",
    "model.add(Input(shape=(d,)))\n",
    "\n",
    "# Hidden layers\n",
    "# Add hidden layer of size 32\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add another hidden layer of size 32\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add one last layer (output) of size \n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model: set up the optimization, loss and metric\n",
    "from keras.optimizers import Adam, SGD\n",
    "lr = 0.001\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=lr), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "b = 32\n",
    "n_epoch = 50\n",
    "val_size = 0.2\n",
    "history = model.fit(X_train, label_train, epochs=n_epoch, batch_size=b, validation_split=val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Learning curves can be extracted from the `history` object.\n",
    "\n",
    "- Create learning curves as shown in [slide 29](https://hassothea.github.io/Data_Analytics_AUPP/slides/Deep_Learning.html#/training-learning-curves).\n",
    "\n",
    "- What information do you get from the curves?\n",
    "\n",
    "- Compute the testing accuracy.\n",
    "\n",
    "- Visualize images that are wrongly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** The network's architecture and hyperparameters were defined nearly at random. Now, it is your turn to try to study the impact of these values.\n",
    "\n",
    "- Increase the number of neurons within each hidden layers.\n",
    "\n",
    "- Change activation function.\n",
    "\n",
    "- Change the learning rate.\n",
    "\n",
    "- Change the optimization algorithm.\n",
    "\n",
    "- Change the minibatch size $b$.\n",
    "\n",
    "In each case, compute the test performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further readings\n",
    "- Graphical tools:\n",
    "    - [`matplotlib`](https://matplotlib.org/stable/index.html)\n",
    "    - [`seaborn`](https://seaborn.pydata.org/)\n",
    "    - [`plotly`](https://plotly.com/python/)\n",
    "- [`Mnist dataset`](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)\n",
    "- [Deep Learning](https://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
